<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>Code Trails — Between Posts: A Quick Update</title>
    <meta name="description" content="Rendered blog post" />
    <style>
      :root { --bg:#ffffff; --fg:#030202; --muted:#6b7280; --divider:#e5e7eb; --maxw: 900px; }
      * { box-sizing: border-box; }
      body { margin:0; font-family: ui-sans-serif,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Helvetica,Arial; color:var(--fg); background:var(--bg); line-height:1.6; }
      .wrap { max-width: var(--maxw); margin: 0 auto; padding: 28px 20px 64px; }
      header h1 { font-size: clamp(28px, 4.5vw, 42px); margin: 6px 0 18px; letter-spacing: -0.01em; }
      .meta { color: var(--muted); font-size: 14px; margin-bottom: 18px; }
      article img { max-width: 100%; height: auto; border-radius: 4px; }
      article pre { background:#0f172a; color:#e2e8f0; padding:12px 14px; overflow:auto; border-radius:6px; }
      article code { background:#f3f4f6; padding: 0 4px; border-radius: 3px; }
      article pre code { background: transparent; padding: 0; color: inherit; }
      article h2 { font-size: 1.6rem; margin-top: 1.8rem; }
      article h3 { font-size: 1.3rem; margin-top: 1.4rem; }
      article h4 { font-size: 1.1rem; margin-top: 1.1rem; }
      article p { margin: 0.7rem 0; }
      article ul { padding-left: 1.2rem; }
      hr { border: 0; border-top: 1px solid var(--divider); margin: 18px 0; }
    </style>
  </head>
  <body>
    <div class="wrap">
      <header>
        <h1 id="post-title">Is Cosine Simularity a Hoax? </h1>
        <div class="meta">By Elsa Hill — September 18, 2025</div>
      </header>
      <article id="post"></article>
    </div>

    <script id="md" type="text/markdown">

## Introduction

When building question‑answering systems, the first step is often to break a long document into smaller pieces, embed them as vectors, and later retrieve the relevant chunks. Different chunking and retrieval strategies can dramatically change which text chunks are surfaced. Initially when I saw results, I concluded cosine simularity wasn't working. However what was later relieveled was it isn't Cosine Simularity, but which embedding models you choose really matters. 

To start, let's check what different types of techniques there are. 

![Chunking Techniques Cheatsheet](blog-images/chunking-techniques-cheatsheet.png)

I used the following query: 

> What are the key findings of this paper?

I evaluated three approaches below:

1. Cosine similarity with a flat index using fixed-size chunking. 

2. HNSW with maximal marginal relevance (MMR) also using fixed-size chunking. 

3. A hybrid method combining bm25 and an e5 encoding model with reranking with dynamic chunking of variable length with aproximate 10% to 20% text overlap. 

This post examines how each method chunks the target documents and the impact on relevance. 

## Data and Methodology

To start, I looked for content I would have a good basis of understanding on. I browsed a lot and finally found something I felt confident about, a thesis on women in computer science. Convienent (so I don't have to dig into a new topic), academic, and the paper's methodology is sound. I stripped out all the math symbols and created a Thesis‑On‑Gender‑In‑CS.txt document. (todo - link to .txt file)

### Approaches at a Glance

- Cosine baseline: fixed‑size windows of 500 characters with 100 characters overlap; sentence‑transformer embeddings; cosine similarity; all chunks kept.

- HNSW + MMR: same fixed windows; embeddings stored in an HNSW index; at query time apply MMR to diversify the top results.

- Hybrid rerank: structure‑aware chunking that respects paragraphs and headings; targets 800–1,200 tokens with ~10–15% overlap; attaches chapter/section metadata; deduplicates near‑duplicates; combines dense+sparse retrieval, expands neighbors, and reranks with a cross‑encoder.

## Chunking Analysis
### Cosine baseline 

I created a vectorstore and used the following embedding model: `sentence-transformers/all-MiniLM-L6-v2`. It neively takes a query, and performs a neirest neighbor search. 

### Chunk 1
--------
```
 may be able to see if there are any significant changes in the categories over time as well as which
 ```

### Chunk 2
--------
```
Economic Sciences. Retrieved from https://www.nsf.gov/statistics/2017/nsf17310/static/downloads/nsf17310-digest.pdf National Student Clearinghouse Research Center. (2017). Undergraduate degree earners report …
```

###  Chunk 3
 --------
 ```
 Implicit Computer Science

 .18

 Identity

 (24)


 .02

 .35

 Identity Imbalance
```

### Results:

>For some reason, the closest 'neighbor' to the query "What are the key findings of this paper?" was the above which didn't extract any relevant chucnks from the source information. 

## HNSW with maximal marginal relevance 

HNSW (Hierarchical Navigable Small World) is built on ideas from graph theory. The “small world” part comes from graph theory, where small-world networks have the property that any node can be reached from any other through only a few hops. [HNSW leverages this by building a multi-layered graph structure.](https://www.pinecone.io/learn/series/faiss/hnsw/) The top layers contain long-range links that make it fast to jump across distant regions of the dataset, while the lower layers contain short-range links that allow for precise navigation among nearby points.

With the `sentence-transformers/all-MiniLM-L6-v2` as the embedding model, let's take a look at the results below:   


### Chunk 1
```
Economic Sciences. Retrieved from … National Student Clearinghouse Research Center. (2017). Undergraduate degree earners report … Nosek, B. A., Greenwald, A. G., & Banaji, M. R. (2005). Understanding and using the implicit association
```

### Chunk 2 
``` 
Testing Greenwald and colleagues’ BID model resulted in significant regressions, suggesting that the IAT and the BID can be used as methods to predict implicit identities. Additionally, due to the BID providing positive results, we are confident in the measures of imbalance …
```
### Chunk 3
```
second step for this model also accounted for four per cent of the variance. Similar to the previous model, science identity and science stereotype were not significant predictors of ethnic identity …
```

## Hybrid retrieval with reranking

The hybrid rerank pipeline takes a different approach with structure‑aware chunking using regex that is analogous to semantic chunking shown above in the [Chunking Techniques Cheatsheet](blog-images/chunking-techniques-cheatsheet.png). It normalises line endings, splits to paragraphs, detects headings, and builds 800–1 200‑token chunks of contiguous paragraphs with ~10–15% overlap while carrying metadata (chapter, section, order, paragraph spans). Quality improvements include:

- Ingest‑time deduplication: drops near‑duplicates with cosine similarity ≥ 0.95.
- BM25 corpus and summaries: tokenises chunks and can add chapter‑level summaries for overview retrieval.
- Neighbour expansion: at query time, appends adjacent chunks around the best hits for continuity.

The result is a smaller, more diverse set of coherent chunks that map to complete ideas.

### Chunk 1
```
Greenwald and colleagues (1998) outlined a five item per category test and a twenty five item per category test. These shorter IATs were selected to account for time. The computer science identity IAT measured the level of association between self and computer science … Positive scores reflect stronger associations toward a computer science identity; negative scores reflect negative associations between self and computer science such that the individual does not identify as a computer scientist. The implicit gender identity test measured how strongly the person associates themselves toward the stated gender.
```

### Chunk 2 
```
Preliminary analyses for mid survey data were conducted to view distributions and descriptive statistics of the variables. Histograms revealed that interest in taking computer science classes had a slight negative skew, interest in a career in computer science had a positive skew, happiness was normally distributed, implicit computer science identity was slightly negatively skewed and identity imbalance was highly skewed, so logarithmic transformations were conducted.
```

### Chunk 3
```
College participants were asked what career they were in and if it was science related. These survey data were compared with additional data requested from the National Student Clearing House. The final analytic sample included one hundred seventy participants who completed the survey items and three IATs measuring ethnic identity, science identity and the ethnic science stereotype. Relationships between these scores were needed to assess identity balance.
```

## Retrieval Quality

- Cosine similarity:
  - Observed: Neirest Neighbor retreval not related to orignal query. 
  - Strengths: simple
  - Weaknesses: poor coherence. 

- HNSW + MMR:
  - Observed: results similiar to neive cosine similarity.
  - Pro: Is a lot quicker than the nieve consine simularity implimentation. 
  - Strengths: Great for very large documents. 

- Hybrid rerank:
  - Observed: Much better chunks related to the paper. 
  - Strengths: combines a traditional ML technique with  and cross‑encoder reranking for relevance and readability.
  - Weaknesses: more complex pipeline; additional preprocessing and compute.

## Pros and Cons Summary

**Cosine baseline:** easiest to set up and can retrieve relevant text, but noisy/overlapping chunks reduce usefulness—better for short documents than long academic texts.

**HNSW with MMR:** adds an efficient index and can be speedy, yet still relies on naive fixed‑length chunks, so returned snippets can be incomplete; redundant content persists in the index.

**Hybrid rerank:** most coherent outputs via structure‑aware chunking, ingest‑time deduplication, hybrid retrieval, diversification, and reranking—at the cost of extra complexity and dependencies.

## Conclusion

When I initially performed a neierest neighbors search, agains the query "What are the key findings of this paper?" I questioned whether Cosine Simularity was a hoax! However upon further realization, the embedding model and design decisions around how the chunking occurs is retrieval‑augmented generation is a key step. 

Naive fixed‑length slicing produces many overlapping, fragmentary chunks, which increases noise and reduces retrieval quality. Ultaimately I found that diversity‑aware retrieval on top of naive chunks helps but doesn’t fix the core issue. Structure‑aware chunking, plus deduplication produces non‑redundant passages that better serve downstream question‑answering for long, complex texts.

      
    </script>

    <script>
      function resolveUrl(url, baseDir) {
        if (/^(https?:)?\/\//i.test(url) || url.startsWith('/') ) return url;
        return baseDir + url.replace(/^\.\//, '');
      }

      function renderMarkdown(md, baseDir) {
        md = md.replace(/[&<>]/g, ch => ({'&':'&amp;','<':'&lt;','>':'&gt;'}[ch]));
        md = md.replace(/\r\n?/g, '\n');
        md = md.replace(/```([\s\S]*?)```/g, (m, code) => '<pre><code>' + code.replace(/</g,'&lt;').replace(/>/g,'&gt;') + '</code></pre>');
        md = md.replace(/`([^`]+)`/g, '<code>$1</code>');
        md = md.replace(/!\[([^\]\n]*)\]\(([^)]+)\)/g, (m, alt, src) => `<img alt="${alt}" src="${resolveUrl(src, baseDir)}">`);
        md = md.replace(/\[([^\]\n]+)\]\(([^)\s]+)\)/g, (m, text, href) => `<a href="${resolveUrl(href, baseDir)}" target="_blank" rel="noopener">${text}</a>`);
        md = md.replace(/\*\*([^*]+)\*\*/g, '<strong>$1</strong>');
        md = md.replace(/\*([^*]+)\*/g, '<em>$1</em>');
        const lines = md.split('\n');
        const out = [];
        let inList = false; let para = [];
        const flushPara = () => { if (para.length) { out.push('<p>' + para.join(' ').trim() + '</p>'); para = []; } };
        for (let i=0;i<lines.length;i++) {
          const line = lines[i];
          if (!line.trim()) { if (inList) { out.push('</ul>'); inList=false; } flushPara(); continue; }
          if (/^---\s*$/.test(line)) { if (inList) { out.push('</ul>'); inList=false; } flushPara(); out.push('<hr>'); continue; }
          if (/^####\s+/.test(line)) { if (inList) { out.push('</ul>'); inList=false; } flushPara(); out.push('<h4>' + line.replace(/^####\s+/, '') + '</h4>'); continue; }
          if (/^###\s+/.test(line))  { if (inList) { out.push('</ul>'); inList=false; } flushPara(); out.push('<h3>' + line.replace(/^###\s+/, '') + '</h3>'); continue; }
          if (/^##\s+/.test(line))   { if (inList) { out.push('</ul>'); inList=false; } flushPara(); out.push('<h2>' + line.replace(/^##\s+/, '') + '</h2>'); continue; }
          const li = line.match(/^\s*-\s+(.*)$/);
          if (li) { flushPara(); if (!inList) { out.push('<ul>'); inList=true; } out.push('<li>' + li[1] + '</li>'); continue; }
          para.push(line.trim());
        }
        if (inList) out.push('</ul>');
        flushPara();
        return out.join('\n');
      }

      (function(){
        const md = document.getElementById('md').textContent;
        const baseDir = '../';
        document.getElementById('post').innerHTML = renderMarkdown(md, baseDir);
      })();
    </script>
  </body>
  </html>
